{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "moral-invite",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "egyptian-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-parameter",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset that I'm going to use for training my hybrid recommender algorithm is going to be the join of the Ratings and Movies tables created during EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "comprehensive-embassy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ratings.csv', sep = ',')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-milan",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "Before introducing the hybrid recommender algorithm it's better to understand what kind of data recommender system algorithms works with. The data for Recommender system algorithms is almost always identical to the table above in place of movieId  it can be bookId, productId et cetera. The feature engineering step here involves the transformation of data into a user-item interaction matrix\n",
    "\n",
    "<blockquote>User-item interaction matrices generally lists users and items in rows and columns, respectively. Then, the interaction records between them are represented as corresponding elements in the matrix</blockquote>\n",
    "\n",
    "\n",
    "There are only two objectives of feature engineering to get data ready for training for this problem\n",
    "## 1)Transform the data into matrix Y (user-item interaction matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-greece",
   "metadata": {},
   "source": [
    "![title](images/Ymatrix.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "stock-antenna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>userId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>610</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193581</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193583</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193585</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193587</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193609</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9724 rows × 610 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "userId   1    2    3    4    5    6    7    8    9    10   ...  601  602  603  \\\n",
       "movieId                                                    ...                  \n",
       "1        4.0  NaN  NaN  NaN  4.0  NaN  4.5  NaN  NaN  NaN  ...  4.0  NaN  4.0   \n",
       "2        NaN  NaN  NaN  NaN  NaN  4.0  NaN  4.0  NaN  NaN  ...  NaN  4.0  NaN   \n",
       "3        4.0  NaN  NaN  NaN  NaN  5.0  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "4        NaN  NaN  NaN  NaN  NaN  3.0  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "5        NaN  NaN  NaN  NaN  NaN  5.0  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "193581   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "193583   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "193585   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "193587   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "193609   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "\n",
       "userId   604  605  606  607  608  609  610  \n",
       "movieId                                     \n",
       "1        3.0  4.0  2.5  4.0  2.5  3.0  5.0  \n",
       "2        5.0  3.5  NaN  NaN  2.0  NaN  NaN  \n",
       "3        NaN  NaN  NaN  NaN  2.0  NaN  NaN  \n",
       "4        NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "5        3.0  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "...      ...  ...  ...  ...  ...  ...  ...  \n",
       "193581   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "193583   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "193585   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "193587   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "193609   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[9724 rows x 610 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MovieUserMatrix = data.pivot_table(index='movieId',columns='userId',values='rating')\n",
    "MovieUserMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "wanted-running",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>userId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>610</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193581</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193583</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193585</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193587</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193609</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9724 rows × 610 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "userId   1    2    3    4    5    6    7    8    9    10   ...  601  602  603  \\\n",
       "movieId                                                    ...                  \n",
       "1        4.0 -1.0 -1.0 -1.0  4.0 -1.0  4.5 -1.0 -1.0 -1.0  ...  4.0 -1.0  4.0   \n",
       "2       -1.0 -1.0 -1.0 -1.0 -1.0  4.0 -1.0  4.0 -1.0 -1.0  ... -1.0  4.0 -1.0   \n",
       "3        4.0 -1.0 -1.0 -1.0 -1.0  5.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0 -1.0   \n",
       "4       -1.0 -1.0 -1.0 -1.0 -1.0  3.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0 -1.0   \n",
       "5       -1.0 -1.0 -1.0 -1.0 -1.0  5.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0 -1.0   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "193581  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0 -1.0   \n",
       "193583  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0 -1.0   \n",
       "193585  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0 -1.0   \n",
       "193587  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0 -1.0   \n",
       "193609  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0 -1.0 -1.0   \n",
       "\n",
       "userId   604  605  606  607  608  609  610  \n",
       "movieId                                     \n",
       "1        3.0  4.0  2.5  4.0  2.5  3.0  5.0  \n",
       "2        5.0  3.5 -1.0 -1.0  2.0 -1.0 -1.0  \n",
       "3       -1.0 -1.0 -1.0 -1.0  2.0 -1.0 -1.0  \n",
       "4       -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "5        3.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "...      ...  ...  ...  ...  ...  ...  ...  \n",
       "193581  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "193583  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "193585  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "193587  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "193609  -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0  \n",
       "\n",
       "[9724 rows x 610 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MovieUserMatrix_Y=MovieUserMatrix.fillna(-1)\n",
    "MovieUserMatrix_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-karen",
   "metadata": {},
   "source": [
    "Transformation of data into matrix Y (user-item interaction matrix) (above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-latter",
   "metadata": {},
   "source": [
    "## 2)Transform the data into matrix R (binary-valued indicator matrix)\n",
    "![title](images/Rmatrix.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "going-chess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>userId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>610</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193581</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193583</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193585</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193587</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193609</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9724 rows × 610 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "userId   1    2    3    4    5    6    7    8    9    10   ...  601  602  603  \\\n",
       "movieId                                                    ...                  \n",
       "1        1.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  ...  1.0  0.0  1.0   \n",
       "2        0.0  0.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  ...  0.0  1.0  0.0   \n",
       "3        1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4        0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "5        0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "193581   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "193583   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "193585   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "193587   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "193609   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "userId   604  605  606  607  608  609  610  \n",
       "movieId                                     \n",
       "1        1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "2        1.0  1.0  0.0  0.0  1.0  0.0  0.0  \n",
       "3        0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "4        0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5        1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...      ...  ...  ...  ...  ...  ...  ...  \n",
       "193581   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "193583   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "193585   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "193587   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "193609   0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[9724 rows x 610 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MovieUserMatrix_R=MovieUserMatrix.copy()\n",
    "MovieUserMatrix_R=MovieUserMatrix_R.where(~MovieUserMatrix_R.notna(), 1)\n",
    "MovieUserMatrix_R=MovieUserMatrix_R.fillna(0)\n",
    "MovieUserMatrix_R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-bargain",
   "metadata": {},
   "source": [
    "Transformation of the data into matrix R (binary-valued indicator matrix) above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "infinite-diesel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "MovieUserMatrix_Y.shape==MovieUserMatrix_R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "paperback-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting values out of pandas dataframe into numpy-array because my alogorithm accept numpy-array as parameter\n",
    "Y=MovieUserMatrix_Y.values\n",
    "R=MovieUserMatrix_R.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-thriller",
   "metadata": {},
   "source": [
    "# Hybrid recommendation approach "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-consultation",
   "metadata": {},
   "source": [
    "The hybrid recommendation system that I'm going to implement consists of the following sequence of steps\n",
    "1. <b>Collaborative filtering</b>:(Used to learn Features for movies)\n",
    "2. <b>Content-based filtering</b>:(Used to learn Parameters unique to web-application user)\n",
    "3. <b>Prediction</b>: (uses both the Features for movies learned using collaborative filtering and the Parameters unique to web-application user learned using content-based filtering to recommend top-N recommendation)\n",
    "![title](images/Hybrid.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-hierarchy",
   "metadata": {},
   "source": [
    "# 1. Collaborative filtering\n",
    "The main objective of collaborative filtering at this step is to learn features for different movies. The implementation of Collaborative filtering here performs \"Feature learning\" Using a variation of multivariate regression with gradient descent as an optimization algorithm, it takes as input user-item interaction matrix and simultaneously learns both the parameters for different users and features for different movies I'm only going to use Features for different movie in the next step but this technique of using both the parameters for users and feature for movies to learn both simultaneously works beautifully for a problem like recommender systems because this approach unlike content-based filtering doesn't require features for different movies to learn parameters for users or parameters for users to learn features for different movies all you need is a user-item interaction matrix and this algorithm will learn a reasonably good set of both the parameters for users and features for movies because it's a modification of multivariate regression whose optimization objective is a concave cost function that always converges towards global minima regardless of your initial values for both the parameters of users and features for different movies. Once features for movies have been learned by collaborative filtering I'm going to save them in python's [.PKL] file that serializes objects to files on disk and deserialized back into the program at runtime when needed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-retail",
   "metadata": {},
   "source": [
    "## Notations\n",
    "* <B>x(i)</B> is a feature vector for movie(i)\n",
    "* <B>θ(j)</B> is a parameter vector for user(j)\n",
    "* The <B>matrix Y</B> (user-item interaction matrix) stores the ratings (from 1 to 5) for all movies(i) by all users(j) where <B>y(i;j)</B> represet the rating for movie(i) by user (j)\n",
    "* The <B>matrix R</B> is a binary-valued indicator matrix, where r(i; j) = 1 if user j gave a rating to movie i, and r(i; j) = 0 otherwise.\n",
    "* <b>nu</b> = total number of users\n",
    "* <b>nm</b> = total number of movies.\n",
    "For this project, I'm using n = 100, and therefore, x(i) ∈ R^100 and θ(j) ∈  R^100.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-hanging",
   "metadata": {},
   "source": [
    "##  Collaborative filtering Cost function\n",
    "The cost function for this Regression-based collaborative filtering algorithm is going to be of the following form:\n",
    "![title](images/costfunc.jpg)\n",
    "\n",
    "\n",
    "## Collaborative filtering Regularized cost function\n",
    "The Regularized cost function for this Regression-based collaborative filtering algorithm is going to be of the following form:\n",
    "![title](images/costfuncRegularized.jpg)\n",
    "\n",
    "## Gradient for Collaborative filtering Cost function\n",
    "The gradient for Collaborative filtering Cost function is as follows:\n",
    "![title](images/costfuncGradient.jpg)\n",
    "\n",
    "## Gradient for Collaborative filtering Regularized Cost function\n",
    "Gradient for Collaborative filtering Regularized Cost function is as follows:\n",
    "![title](images/costfuncGradientRegularized.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "separated-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of Collaborative filtering Cost function and gradient, regularized and non regularized\n",
    "def  cofiCostFunc(params, Y, R, num_users, num_movies, num_features, Lambda):\n",
    "    \"\"\"\n",
    "            calculates Collaborative filtering Cost function and gradient both regularized and non regularized\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            params: numpy array-like\n",
    "                    initial values for users parameter vectors and movies feature vectors\n",
    "            \n",
    "            Y: numpy array-like\n",
    "                    user-item interaction matrix\n",
    "            R: numpy arra-like\n",
    "                    Its a binary-valued indicator matrix for user-item interaction matrix\n",
    "            num_users: int-like\n",
    "                    total number of users\n",
    "            num_movies: int-like\n",
    "                    total number of movies\n",
    "            Lambda: Float-like\n",
    "                    Regularization parameter\n",
    "                    \n",
    "            Returns\n",
    "            -------\n",
    "            J:Float-like\n",
    "                    Cost\n",
    "            grad:Float-like\n",
    "                    Gradient\n",
    "            reg_J:Float-like\n",
    "                    Regularized Cost\n",
    "            reg_grad:Float-like\n",
    "                    Regularized gradient\n",
    "    \"\"\"\n",
    "    # Unfold the params\n",
    "    X = params[:num_movies*num_features].reshape(num_movies,num_features)\n",
    "    Theta = params[num_movies*num_features:].reshape(num_users,num_features)\n",
    "    \n",
    "    predictions =  np.dot(X,Theta.T)\n",
    "    err = (predictions - Y)\n",
    "    J = 1/2 * np.sum((err**2) * R)# multiplying by R will consider only those values for which value of (i,j)=r(i,j)=1\n",
    "    \n",
    "    #compute regularized cost function\n",
    "    reg_X =  Lambda/2 * np.sum(Theta**2)\n",
    "    reg_Theta = Lambda/2 *np.sum(X**2)\n",
    "    reg_J = J + reg_X + reg_Theta\n",
    "    \n",
    "    # Compute gradient\n",
    "    X_grad = np.dot(err*R,Theta)\n",
    "    Theta_grad = np.dot((err*R).T,X)\n",
    "    grad = np.append(X_grad.flatten(),Theta_grad.flatten()) #flattening calculated gradients of 4*3 dim into 12 real number one dim of vector for optimization algo since optimization algorithms deals with vectos not matrix \n",
    "    \n",
    "    # Compute regularized gradient\n",
    "    reg_X_grad = X_grad + Lambda*X\n",
    "    reg_Theta_grad = Theta_grad + Lambda*Theta\n",
    "    reg_grad = np.append(reg_X_grad.flatten(),reg_Theta_grad.flatten())\n",
    "    #, reg_J, reg_grad\n",
    "    \n",
    "    return J, grad, reg_J, reg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-selection",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "Numerical optimization algorithm to minimize the Collaborative filtering Cost function By noting α ∈ R the learning rate, the update rule for gradient descent\n",
    "is expressed with the learning rate and the cost function J as follows:\n",
    "![title](images/GD.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "false-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient descent implementation\n",
    "def gradientDescent(initial_parameters,Y,R,num_users,num_movies,num_features,alpha,num_iters,Lambda):\n",
    "    \"\"\"\n",
    "            Optimize X and Theta\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            initial_parameters: numpy array-like\n",
    "                    initial values for users parameter vectors and movies feature vectors\n",
    "            \n",
    "            Y: numpy array-like\n",
    "                    user-item interaction matrix\n",
    "            R: numpy arra-like\n",
    "                    Its a binary-valued indicator matrix for user-item interaction matrix\n",
    "            num_users: int-like\n",
    "                    total number of users\n",
    "            num_movies: int-like\n",
    "                    total number of movies\n",
    "            alpha: Float-lik\n",
    "                    learning rate\n",
    "            num_iters: integer-like\n",
    "                    number of iterations of optimization algorithm\n",
    "            Lambda: Float-like\n",
    "                    Regularization parameter\n",
    "                    \n",
    "            Returns\n",
    "            -------\n",
    "            paramsFinal: numpy-array-like\n",
    "                    Learned user parameters and feature vectors for movies\n",
    "            J_history: numpy-array-like\n",
    "                     History of decrease in cost as gradeint descent moves towords global minima\n",
    "    \"\"\"\n",
    "    # unfold the parameters\n",
    "    X = initial_parameters[:num_movies*num_features].reshape(num_movies,num_features)\n",
    "    Theta = initial_parameters[num_movies*num_features:].reshape(num_users,num_features)\n",
    "    \n",
    "    J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        params = np.append(X.flatten(),Theta.flatten())\n",
    "        cost, grad = cofiCostFunc(params, Y, R, num_users, num_movies, num_features, Lambda)[2:]\n",
    "        \n",
    "        # unfold grad\n",
    "        X_grad = grad[:num_movies*num_features].reshape(num_movies,num_features)\n",
    "        Theta_grad = grad[num_movies*num_features:].reshape(num_users,num_features)\n",
    "        X = X - (alpha * X_grad)\n",
    "        Theta = Theta - (alpha * Theta_grad)\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    paramsFinal = np.append(X.flatten(),Theta.flatten())\n",
    "    return paramsFinal , J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "speaking-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRatings(Y, R):\n",
    "    \"\"\"\n",
    "    normalized Y so that each movie has a rating of 0 on average, and returns the mean rating in Ymean.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y: numpy array-like\n",
    "                    user-item interaction matrix\n",
    "    R: numpy arra-like\n",
    "                    Its a binary-valued indicator matrix for user-item interaction matrix\n",
    "    Ynorm:numpy array-like\n",
    "                    Normalize Y\n",
    "    Ymean:numpy array-like\n",
    "                    Mean of all movies\n",
    "    \"\"\"\n",
    "    \n",
    "    m,n = Y.shape[0], Y.shape[1]\n",
    "    Ymean = np.zeros((m,1))\n",
    "    Ynorm = np.zeros((m,n))\n",
    "    \n",
    "    for i in range(m):\n",
    "        Ymean[i] = np.sum(Y[i,:])/np.count_nonzero(R[i,:])\n",
    "        Ynorm[i,R[i,:]==1] = Y[i,R[i,:]==1] - Ymean[i]\n",
    "    return Ynorm, Ymean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-advocate",
   "metadata": {},
   "source": [
    "# 2. Content-based filtering\n",
    "The content-based filtering here again is going to be an extension of multivariate regression but unlike collaborative filtering here I'm going to use the features for movies learned using collaborative filtering now to learn online web-application user parameter using content-based filtering thats unique to the user based on his/her web application movie ratings.This content-based filtering algorithm will run and learn online the parameters unique to each web-application user  but this algorithm by no means a type of Online learning algorithm because that definition strictly includes the ability of algorithm to use sequential data coming as input online and use it to update the best predictor for future data at each step\n",
    "\n",
    "## Notations\n",
    "* <B>x(i)</B> is a feature vector for movie(i)\n",
    "* <B>θ</B> is a parameter vector for web-applicaiton user\n",
    "* <b>y(i)</b> i the rating for movie i by web-applicaiton user\n",
    "* r(i) = 1 if web-applicaiton user gave a rating to movie i, and r(i) = 0 otherwise.\n",
    "\n",
    ".\n",
    "I'm using n = 100, and therefore, x(i) ∈ R^100 and θ ∈  R^100.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-enlargement",
   "metadata": {},
   "source": [
    "## Content-based filtering Cost function\n",
    "![title](images/CBcost.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "widespread-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta):\n",
    "    \"\"\"\n",
    "            calculates cost for given value of θ \n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            X: numpy-array-like\n",
    "               matrix of features for all movies\n",
    "               \n",
    "            y: numpy-array-like\n",
    "               binary vector of web-application user movie ratings (y ∈ R^9742) where 9742 is the number of movies rated by web-app user\n",
    "                    \n",
    "            theta:numpy-array-like\n",
    "               value of θ for web application user\n",
    "               \n",
    "            Returns\n",
    "            -------\n",
    "            j: float-like\n",
    "               cost\n",
    "    \"\"\"\n",
    "    m=y.size\n",
    "    s=np.dot(X,theta)-y\n",
    "    j=(1/(2*m))*(np.dot(np.transpose(s),s))\n",
    "    return j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-spice",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "Numerical optimization algorithm to minimize the Content-based filtering  Cost function By noting α ∈ R the learning rate, the update rule for gradient descent is expressed with the learning rate and the cost function J as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-counter",
   "metadata": {},
   "source": [
    "![title](images/CF.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "removable-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CBgradientDescent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn `theta`. Updates theta by taking `num_iters`\n",
    "    gradient steps with learning rate `alpha`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        matrix of features for all movies\n",
    "    \n",
    "    y : array_like\n",
    "        binary vector of ∈ R^9742 movies where 9742 is the number of movies\n",
    "    \n",
    "    theta : array_like\n",
    "        Initial values for the web-applicaiton user parameters. \n",
    "    \n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "    \n",
    "    num_iters : int\n",
    "        The number of iterations for gradient descent. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : array_like\n",
    "        The learned parameters. \n",
    "    \"\"\"\n",
    "    m = float(y.shape[0])\n",
    "    theta = theta.copy()\n",
    "    for i in range(num_iters):\n",
    "        theta=(theta)-(alpha/m)*(np.dot(np.transpose((np.dot(X,theta)-y)),X))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-nancy",
   "metadata": {},
   "source": [
    "# 3. Prediction\n",
    "Prediction (uses both the Features for movies learned using collaborative filtering and the Parameters unique to user learned using content-based filtering to recommend top-N recommendation)\n",
    "The prediction uses both the vectors for movies learned using collaborative filtering and the parameter unique to user learned using content-based filtering to recommend top-N recommendation\n",
    "\n",
    "The  prediction logic for my web application user is pretty straightforward I'm going to predict the score for all 9742 movies, where each prediction is a linear combination of Movie feature vector (x) learned using Collaborative filtering  and Feature vector for web-application user θ  learned using content-based filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-frank",
   "metadata": {},
   "source": [
    "![title](images/predi.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "simplified-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X,my_ratings,moviesdataset):\n",
    "    \"\"\"\n",
    "    Performs prediction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        matrix of features for all movies\n",
    "    \n",
    "    my_ratings : numpy-array-like\n",
    "        Binary vector of web-application user movie ratings (y ∈ R^9742) where 9742 is the number of movies rated by web-app user\n",
    "                    \n",
    "    moviesdataset : pandas dataframe-like\n",
    "        Dataframe contains movieid and title for all 9742 movies\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sorted_data : array_like\n",
    "        top-N recommendation\n",
    "    \n",
    "    \"\"\"\n",
    "    out_arr = my_ratings[np.nonzero(my_ratings)]\n",
    "    out_arr=out_arr.reshape(-1,1)\n",
    "    idx = np.where(my_ratings)[0]\n",
    "    X_1=[X[x] for x in idx]\n",
    "    X_1=np.array(X_1)\n",
    "    y=out_arr\n",
    "    y=np.reshape(y, -1)\n",
    "    theta =CBgradientDescent(X_1,y,np.zeros((100)),0.001,4000)\n",
    "    #mean=np.reshape(Ymean, -1)\n",
    "    p = X @ theta.T\n",
    "    #p=p+mean\n",
    "    p=np.reshape(p, -1)\n",
    "    predictedData=moviesdataset.copy()\n",
    "    predictedData['Pridiction']=p\n",
    "    sorted_data=predictedData.sort_values(by=['Pridiction'],ascending=False)\n",
    "    return sorted_data[:40]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-psychology",
   "metadata": {},
   "source": [
    "# Training and Evaluation of Recommendation Model\n",
    "Unlike classification and regression problems, there is no hard-and-fast rule when it comes to the evaluation of recommender systems At the end of the day, system designer employing a recommendation system must choose between a set of candidate approaches \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-stock",
   "metadata": {},
   "source": [
    "## Train-Test split\n",
    "For Train-Test split, there are again numerous techniques available [this](https://arxiv.org/pdf/2007.13237.pdf) paper is a great read on this subject for this project I will split my data into training and test sets by removing 10 ratings per user from the training set and placing them in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "above-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(Yratings,Rratings):\n",
    "    \"\"\"\n",
    "    train and test split\n",
    "    Parameters\n",
    "    ----------\n",
    "    Yratings: numpy array-like\n",
    "                    user-item interaction matrix\n",
    "    Rratings: numpy arra-like\n",
    "                    Its a binary-valued indicator matrix for user-item interaction matrix\n",
    "                    \n",
    "    Returns\n",
    "    -------\n",
    "    Ytrain: numpy array-like\n",
    "                   train user-item interaction matrix\n",
    "    Ytest:  numpy array-like\n",
    "                   test user-item interaction matrix\n",
    "    Rtrain: numpy array-like\n",
    "                   train its a binary-valued indicator matrix for user-item interaction matrix\n",
    "    Rtest:  numpy array-like\n",
    "                   test its a binary-valued indicator matrix for user-item interaction matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    Ytest = np.zeros(Yratings.shape)\n",
    "    Ytrain = Yratings.copy()\n",
    "    Rtest = np.zeros(Rratings.shape)\n",
    "    Rtrain = Rratings.copy()\n",
    "    \n",
    "    for user in range(Yratings.shape[0]):\n",
    "        try:\n",
    "            test_ratings = np.random.choice(Yratings[user, :].nonzero()[0], \n",
    "                                            size=10, \n",
    "                                            replace=False)\n",
    "            Ytrain[user, test_ratings] = 0.\n",
    "            Ytest[user, test_ratings] = Yratings[user, test_ratings]\n",
    "            \n",
    "            Rtrain[user, test_ratings] = 0.\n",
    "            Rtest[user, test_ratings] = Rratings[user, test_ratings]\n",
    "        except ValueError:\n",
    "            test_ratings = np.random.choice(Yratings[user, :].nonzero()[0], \n",
    "                                            size=10, \n",
    "                                            replace=True)\n",
    "            Ytrain[user, test_ratings] = 0.\n",
    "            Ytest[user, test_ratings] = Yratings[user, test_ratings]\n",
    "            \n",
    "            Rtrain[user, test_ratings] = 0.\n",
    "            Rtest[user, test_ratings] = Rratings[user, test_ratings]\n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((Ytrain * Ytest) == 0))\n",
    "    assert(np.all((Rtrain * Rtest) == 0))\n",
    "    return Ytrain, Ytest, Rtrain, Rtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "driven-equipment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:18: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\T'\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_24040\\2327504608.py:18: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  plt.ylabel(\"$J(\\Theta)$\")\n"
     ]
    }
   ],
   "source": [
    "#Train-Test split\n",
    "Ytrain, Ytest, Rtrain, Rtest = train_test_split(Y,R)\n",
    "num_users = Ytrain.shape[1]\n",
    "num_movies = Ytrain.shape[0]\n",
    "num_features = 100\n",
    "\n",
    "# Set initial Parameters (Theta,X)\n",
    "X = np.random.randn(num_movies, num_features)\n",
    "Theta = np.random.randn(num_users, num_features)\n",
    "initial_parameters = np.append(X.flatten(),Theta.flatten())\n",
    "Lambda = 10\n",
    "\n",
    "# learns Features for movies and parameters for all users using Collaborative filtering\n",
    "paramsFinal, J_history = gradientDescent(initial_parameters,Ytrain,Rtrain,num_users,num_movies,num_features,0.001,1000,Lambda)\n",
    "\n",
    "plt.plot(J_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\Theta)$\")\n",
    "plt.title(\"Cost function using Gradient Descent\")\n",
    "X = paramsFinal[:num_movies*num_features].reshape(num_movies,num_features)\n",
    "Theta = paramsFinal[num_movies*num_features:].reshape(num_users,num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-small",
   "metadata": {},
   "source": [
    "The above curve informs that the Collaborative filtering is indeed working and with each iteration of Gradient descent the cost of my cost function is going down and plateaued out around 200 and from there onwards the change is constant "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-trinidad",
   "metadata": {},
   "source": [
    "## Learning Curve Analysis\n",
    "For the evaluation of recommender systems, I'm going to use the MSE metric but just calculating MSE  alone for both train and test set means nothing I need to perform a learning curve analysis to better understand the quality of my model performance to that end I will train my model on different sizes of train set and evaluate both the training and test set MSE score for a given size of train and test set and let the learning curves guide me to the further improvement of my model based on the result of the learning curve my model can be suffering from bias problem, variance problem or if I'm lucky enough then I will in goldilock zone which is the ideal situation.\n",
    "\n",
    "\n",
    "<b>NOTE</b>: the dataset used in this notebook is the smallest available on  MovieLens of only 1 MB this one is only intended for educational purposes hence the learning curves are not going to be ideal by any means but I've trained the final model that I've deployed on 265 MB dataset from movieLens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(Ytrain,Rtrain,Ytest,Rtest):\n",
    "    \"\"\"\n",
    "    compute data for learning Curve\n",
    "                    \n",
    "    Parameters\n",
    "    -------\n",
    "    Ytrain: numpy array-like\n",
    "                   train user-item interaction matrix\n",
    "    Ytest:  numpy array-like\n",
    "                   test user-item interaction matrix\n",
    "    Rtrain: numpy array-like\n",
    "                   train its a binary-valued indicator matrix for user-item interaction matrix\n",
    "    Rtest:  numpy array-like\n",
    "                   test its a binary-valued indicator matrix for user-item interaction matrix\n",
    "                   \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    train_size_hist: numpy array-like\n",
    "                   different training set sizes used\n",
    "    train_score_hist: numpy array-like\n",
    "                   scores relative to training size\n",
    "    test_size_hist: numpy array-like\n",
    "                   different test set sizes used\n",
    "    test_score_hist: numpy array-like\n",
    "                   scores relative to training size\n",
    "    \n",
    "    \"\"\"\n",
    "    Y=Ytrain\n",
    "    R=Rtrain\n",
    "    test_Y=Ytest\n",
    "    test_R=Rtest\n",
    "    train_size_hist=[]\n",
    "    train_score_hist=[]\n",
    "    test_size_hist=[]\n",
    "    test_score_hist=[]\n",
    "    size_list=[1,100,800,1000,1500,2000,2500,3000,3300,3800,4800,5900,6500,7000,7600,8300,9724]\n",
    "    for size in size_list:\n",
    "        Ytrain=Y[:size]\n",
    "        Rtrain=R[:size]\n",
    "        Ytest=test_Y[:size]\n",
    "        Rtest=test_R[:size]\n",
    "        num_users = Ytrain.shape[1]\n",
    "        num_movies = Ytrain.shape[0]\n",
    "        num_features = 100\n",
    "\n",
    "        # Set initial Parameters (Theta,X)\n",
    "        X = np.random.randn(num_movies, num_features)\n",
    "        Theta = np.random.randn(num_users, num_features)\n",
    "        initial_parameters = np.append(X.flatten(),Theta.flatten())\n",
    "        Lambda = 10\n",
    "\n",
    "        # Optimize parameters using Gradient Descent\n",
    "        paramsFinal, J_history = gradientDescent(initial_parameters,Ytrain,Rtrain,num_users,num_movies,num_features,0.001,1000,Lambda)\n",
    "        X = paramsFinal[:num_movies*num_features].reshape(num_movies,num_features)\n",
    "        Theta = paramsFinal[num_movies*num_features:].reshape(num_users,num_features)\n",
    "        pred = X @ Theta.T\n",
    "        pred=pred*Rtrain\n",
    "        actual=Ytrain\n",
    "        train_size_hist.append(size)\n",
    "        train_score_hist.append(get_mse(pred, actual))\n",
    "        pred = X @ Theta.T\n",
    "        pred=pred*Rtest\n",
    "        actual=Ytest\n",
    "        test_size_hist.append(size)\n",
    "        test_score_hist.append(get_mse(pred, actual))\n",
    "    return train_size_hist,train_score_hist,test_size_hist,test_score_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_hist,train_score_hist,test_size_hist,test_score_hist=learningCurve(Ytrain,Rtrain,Ytest,Rtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-mathematics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.97, 1.05)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_size_hist, train_score_hist, label = 'Training error')\n",
    "plt.plot(train_size_hist, test_score_hist, label = 'Validation error')\n",
    "plt.ylabel('MSE', fontsize = 18)\n",
    "plt.yticks(np.arange(0,2, 0.25))\n",
    "plt.xlabel('Training set size', fontsize = 18)\n",
    "plt.title('Learning curves for a Collaborative Filtering', fontsize = 18, y = 1.03)\n",
    "plt.legend()\n",
    "plt.ylim(0.97,1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-prisoner",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "As I mentioned earlier the dataset used in this notebook is the smallest available on  MovieLens of only 1 MB this one is only intended for educational purposes hence these learning curves are not ideal by any means more specifically the gap between the two learning curves suggests a substantial increase in variance. The low training MSEs corroborate this diagnosis of high variance. The large gap and the low training error also indicate an overfitting problem. Overfitting happens when the model performs well on the training set, but far poorer on the test (or validation) set. One more important observation we can make here is that adding new training instances is very likely to lead to a better model that's exactly what I did and trained my model for the 265 MB dataset from movieLens. another thing to be noted here is the sizes used for learning curves are randomly cherry-picked by me if the learning curve analysis is performed for every possible size the learning curves will be much smoother\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-forth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>964982703</td>\n",
       "      <td>toy story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1487954343</td>\n",
       "      <td>toy story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1316196157</td>\n",
       "      <td>toy story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1533872400</td>\n",
       "      <td>toy story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>853937855</td>\n",
       "      <td>toy story (1995)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId   timestamp             title\n",
       "0        1   964982703  toy story (1995)\n",
       "1        1  1487954343  toy story (1995)\n",
       "2        1  1316196157  toy story (1995)\n",
       "3        1  1533872400  toy story (1995)\n",
       "4        1   853937855  toy story (1995)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning and transforming moviesDataset to use it on web-application \n",
    "moviesdataset=data.drop(['userId','rating'],axis=1)\n",
    "moviesdataset=moviesdataset.drop_duplicates()\n",
    "moviesdataset=moviesdataset.sort_values(\"movieId\")\n",
    "moviesdataset=moviesdataset.reset_index(drop=True)\n",
    "moviesdataset[\"title\"] = moviesdataset[\"title\"].str.lower() \n",
    "moviesdataset[\"title\"] = moviesdataset[\"title\"].str.replace(r\"\\(.*\\)\",\"\")\n",
    "moviesdataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-force",
   "metadata": {},
   "source": [
    "## Testing \n",
    "Now at this point, I'm all set to start the <b> flask web application</b>  development but before moving onwards I'm just going to test if everything working with my content-based filtering algorithm for that end I'm going to test my content-based filtering algorithm out\n",
    "I've already learned the feature vectors for all movie using a collaborative filtering algorithm now as a new user on web application rates a handful of movies my content-based filtering algorithm will use learned the feature vectors for all movies using collaborative filtering algorithm and ratings by the web application user to learn parameter unique to this web-application user and then perform the linear combination prediction logic to recommend top-N-movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-delivery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user ratings:\n",
      "\n",
      "Rated 4 for Movie\n",
      "toy story (1995)\n",
      "Rated 3 for Movie\n",
      "toy story (1995)\n",
      "Rated 5 for Movie\n",
      "toy story (1995)\n",
      "Rated 4 for Movie\n",
      "toy story (1995)\n",
      "Rated 5 for Movie\n",
      "toy story (1995)\n",
      "Rated 3 for Movie\n",
      "toy story (1995)\n",
      "Rated 5 for Movie\n",
      "toy story (1995)\n",
      "Rated 4 for Movie\n",
      "toy story (1995)\n",
      "Rated 2 for Movie\n",
      "toy story (1995)\n",
      "Rated 5 for Movie\n",
      "jumanji (1995)\n",
      "Rated 5 for Movie\n",
      "grumpier old men (1995)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_7036\\1848918590.py:20: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"Rated\",int(my_ratings[i]),\"for Movie\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize my ratings\n",
    "my_ratings = np.zeros((100836,1))\n",
    "\n",
    "# test rating by web-applicaiton user\n",
    "my_ratings[0] = 4 \n",
    "my_ratings[97] = 2\n",
    "my_ratings[6] = 3\n",
    "my_ratings[11]= 5\n",
    "my_ratings[53] = 4\n",
    "my_ratings[63]= 5\n",
    "my_ratings[65]= 3\n",
    "my_ratings[68] = 5\n",
    "my_ratings[82]= 4\n",
    "my_ratings[225] = 5\n",
    "my_ratings[354]= 5\n",
    "\n",
    "print(\"New user ratings:\\n\")\n",
    "for i in range(len(my_ratings)):\n",
    "    if my_ratings[i]>0:\n",
    "        print(\"Rated\",int(my_ratings[i]),\"for Movie\")\n",
    "        print((moviesdataset.iloc[i]).title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-passenger",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (9724) does not match length of index (100836)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# top-N-movies\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mprediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmy_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmoviesdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 35\u001b[0m, in \u001b[0;36mprediction\u001b[1;34m(X, my_ratings, moviesdataset)\u001b[0m\n\u001b[0;32m     33\u001b[0m p\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mreshape(p, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m predictedData\u001b[38;5;241m=\u001b[39mmoviesdataset\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 35\u001b[0m \u001b[43mpredictedData\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPridiction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39mp\n\u001b[0;32m     36\u001b[0m sorted_data\u001b[38;5;241m=\u001b[39mpredictedData\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPridiction\u001b[39m\u001b[38;5;124m'\u001b[39m],ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sorted_data[:\u001b[38;5;241m40\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4530\u001b[0m     ):\n\u001b[0;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (9724) does not match length of index (100836)"
     ]
    }
   ],
   "source": [
    "# top-N-movies\n",
    "prediction(X,my_ratings,moviesdataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
